{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssCOanHc8JH_"
   },
   "source": [
    "# Training in Brax\n",
    "\n",
    "Once an environment is created in brax, we can quickly train it using brax's built-in training algorithms. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 22308,
     "status": "ok",
     "timestamp": 1679686324614,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "kUrAlZTod7t_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded on NVIDIA RTX A6000 device.\n"
     ]
    }
   ],
   "source": [
    "#@markdown ## âš ï¸ PLEASE NOTE:\n",
    "#@markdown This colab runs best using a GPU runtime.  From the Colab menu, choose Runtime > Change Runtime Type, then select **'GPU'** in the dropdown.\n",
    "\n",
    "import functools\n",
    "import jax\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from jax import numpy as jp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML, clear_output,display, IFrame\n",
    "\n",
    "import brax\n",
    "\n",
    "\n",
    "import flax\n",
    "from brax import envs\n",
    "from brax.io import model\n",
    "from brax.io import json\n",
    "from brax.io import html\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.sac import train as sac\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  from jax.tools import colab_tpu\n",
    "  colab_tpu.setup_tpu()\n",
    "\n",
    "print(\"loaded on\", jax.devices()[0].device_kind, \"device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoyyw6pQ3xVJ"
   },
   "source": [
    "First let's pick an environment and a backend to train an agent in. \n",
    "\n",
    "Recall from the [Brax Basics](https://github.com/google/brax/blob/main/notebooks/basics.ipynb) colab, that the backend specifies which physics engine to use, each with different trade-offs between physical realism and training throughput/speed. The engines generally decrease in physical realism but increase in speed in the following order: `generalized`,  `positional`, then `spring`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "height": 480
    },
    "executionInfo": {
     "elapsed": 6143,
     "status": "ok",
     "timestamp": 1679686333304,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "4hHuDp53e4VJ",
    "outputId": "90d4c41b-9a25-4ca4-c6be-93172ea9ef57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacealdr/.local/lib/python3.10/site-packages/mujoco/mjx/_src/mesh.py:177: UserWarning: Mesh \"link1_c\" has a coplanar face with more than 20 vertices. This may lead to performance issues and inaccuracies in collision detection. Consider decimating the mesh.\n",
      "  warnings.warn(\n",
      "/home/jacealdr/.local/lib/python3.10/site-packages/mujoco/mjx/_src/mesh.py:177: UserWarning: Mesh \"link2_c\" has a coplanar face with more than 20 vertices. This may lead to performance issues and inaccuracies in collision detection. Consider decimating the mesh.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#@title Load Env { run: \"auto\" }\n",
    "\n",
    "env_name = 'franka'  # @param ['ant', 'halfcheetah', 'hopper', 'humanoid', 'humanoidstandup', 'inverted_pendulum', 'inverted_double_pendulum', 'pusher', 'reacher', 'walker2d']\n",
    "backend = 'positional'  # @param ['generalized', 'positional', 'spring']\n",
    "\n",
    "env = envs.get_environment(env_name=env_name,\n",
    "                           backend=backend)\n",
    "state = jax.jit(env.reset)(rng=jax.random.PRNGKey(seed=0))\n",
    "\n",
    "\n",
    "# # display(HTML(html.render(env.sys, [state.pipeline_state])))\n",
    "# print(html.render(env.sys, [state.pipeline_state]))\n",
    "# pre_render = html.render(env.sys, [state.pipeline_state])\n",
    "# # wrapped_render = f\"<iframe sandbox='allow-scripts allow-forms' style='display:block; margin:0px;' frameborder='0' srcdoc='{pre_render}' width='100%' height='500px'></iframe>\"\n",
    "# # display(HTML(wrapped_render))\n",
    "# HTML(pre_render)\n",
    "# # HTML('<img src=\"reward.png')\n",
    "# display(HTML(html.render(env.sys, [state.pipeline_state])))\n",
    "# save html to file\n",
    "html_file = f\"{env_name}.html\"\n",
    "with open(html_file, 'w') as f:\n",
    "  f.write(html.render(env.sys, [state.pipeline_state],height=1600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMailSDb30t-"
   },
   "source": [
    "# Training\n",
    "\n",
    "Brax provides out of the box the following training algorithms:\n",
    "\n",
    "* [Proximal policy optimization](https://github.com/google/brax/blob/main/brax/training/agents/ppo/train.py)\n",
    "* [Soft actor-critic](https://github.com/google/brax/blob/main/brax/training/agents/sac/train.py)\n",
    "* [Evolutionary strategy](https://github.com/google/brax/blob/main/brax/training/agents/es/train.py)\n",
    "* [Analytic policy gradients](https://github.com/google/brax/blob/main/brax/training/agents/apg/train.py)\n",
    "* [Augmented random search](https://github.com/google/brax/blob/main/brax/training/agents/ars/train.py)\n",
    "\n",
    "Trainers take as input an environment function and some hyperparameters, and return an inference function to operate the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3MA1UYlftuq"
   },
   "source": [
    "# Training\n",
    "\n",
    "Let's train the Ant policy using the `generalized` backend with PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "height": 321
    },
    "executionInfo": {
     "elapsed": 190263,
     "status": "ok",
     "timestamp": 1671658344336,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "FB6G2_Yt4A2m",
    "outputId": "402a0a43-3525-4eca-a425-ffcc71e8db0f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#@title Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# We determined some reasonable hyperparameters offline and share them here.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_fn \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minverted_pendulum\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minverted_double_pendulum\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mant\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain,  num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoid\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain,  num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreacher\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, max_devices_per_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoidstandup\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.97\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhopper\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(sac\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6_553_600\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.997\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, grad_updates_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, max_devices_per_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_replay_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1048576\u001b[39m, min_replay_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalker2d\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(sac\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7_864_320\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.997\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6e-4\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, grad_updates_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_devices_per_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_replay_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1048576\u001b[39m, min_replay_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalfcheetah\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpusher\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000_000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m,entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfranka\u001b[39m\u001b[38;5;124m'\u001b[39m: functools\u001b[38;5;241m.\u001b[39mpartial(ppo\u001b[38;5;241m.\u001b[39mtrain, num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, num_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, reward_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, normalize_observations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, unroll_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_minibatches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_updates_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, discounting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, entropy_cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, max_devices_per_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     16\u001b[0m   \n\u001b[0;32m---> 17\u001b[0m }[\u001b[43menv_name\u001b[49m]\n\u001b[1;32m     20\u001b[0m max_y \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mant\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalfcheetah\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhopper\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2500\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m13000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoidstandup\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m75_000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreacher\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfranka\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalker2d\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpusher\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}[env_name]\n\u001b[1;32m     21\u001b[0m min_y \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreacher\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpusher\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m150\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfranka\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m}\u001b[38;5;241m.\u001b[39mget(env_name, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_name' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Training\n",
    "\n",
    "# We determined some reasonable hyperparameters offline and share them here.\n",
    "train_fn = {\n",
    "  'inverted_pendulum': functools.partial(ppo.train, num_timesteps=2_000_000, num_evals=20, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
    "  'inverted_double_pendulum': functools.partial(ppo.train, num_timesteps=20_000_000, num_evals=20, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
    "  'ant': functools.partial(ppo.train,  num_timesteps=50_000_000, num_evals=10, reward_scaling=10, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=5, num_minibatches=32, num_updates_per_batch=4, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-2, num_envs=4096, batch_size=2048, seed=1),\n",
    "  'humanoid': functools.partial(ppo.train,  num_timesteps=50_000_000, num_evals=10, reward_scaling=0.1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=10, num_minibatches=32, num_updates_per_batch=8, discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=1024, seed=1),\n",
    "  'reacher': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=4, unroll_length=50, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=256, max_devices_per_host=8, seed=1),\n",
    "  'humanoidstandup': functools.partial(ppo.train, num_timesteps=100_000_000, num_evals=20, reward_scaling=0.1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=15, num_minibatches=32, num_updates_per_batch=8, discounting=0.97, learning_rate=6e-4, entropy_cost=1e-2, num_envs=2048, batch_size=1024, seed=1),\n",
    "  'hopper': functools.partial(sac.train, num_timesteps=6_553_600, num_evals=20, reward_scaling=30, episode_length=1000, normalize_observations=True, action_repeat=1, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=512, grad_updates_per_step=64, max_devices_per_host=1, max_replay_size=1048576, min_replay_size=8192, seed=1),\n",
    "  'walker2d': functools.partial(sac.train, num_timesteps=7_864_320, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=1, discounting=0.997, learning_rate=6e-4, num_envs=128, batch_size=128, grad_updates_per_step=32, max_devices_per_host=1, max_replay_size=1048576, min_replay_size=8192, seed=1),\n",
    "  'halfcheetah': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=1, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=20, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=0.001, num_envs=2048, batch_size=512, seed=3),\n",
    "  'pusher': functools.partial(ppo.train, num_timesteps=50_000_000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=1, unroll_length=30, num_minibatches=16, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4,entropy_cost=1e-2, num_envs=2048, batch_size=512, seed=3),\n",
    "  'franka': functools.partial(ppo.train, num_timesteps=1000, num_evals=20, reward_scaling=5, episode_length=1000, normalize_observations=True, action_repeat=4, unroll_length=50, num_minibatches=32, num_updates_per_batch=8, discounting=0.95, learning_rate=3e-4, entropy_cost=1e-3, num_envs=2048, batch_size=256, max_devices_per_host=8, seed=1),\n",
    "}[env_name]\n",
    "\n",
    "\n",
    "max_y = {'ant': 8000, 'halfcheetah': 8000, 'hopper': 2500, 'humanoid': 13000, 'humanoidstandup': 75_000, 'reacher': 5, 'franka':5, 'walker2d': 5000, 'pusher': 0}[env_name]\n",
    "min_y = {'reacher': -100, 'pusher': -150, 'franka': -100}.get(env_name, 0)\n",
    "\n",
    "xdata, ydata = [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  times.append(datetime.now())\n",
    "  xdata.append(num_steps)\n",
    "  ydata.append(metrics['eval/episode_reward'])\n",
    "  clear_output(wait=True)\n",
    "  plt.xlim([0, train_fn.keywords['num_timesteps']])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.plot(xdata, ydata)\n",
    "  plt.show()\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(environment=env, progress_fn=progress)\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjlh7puy2ZM1"
   },
   "source": [
    "The trainers return an inference function, parameters, and the final set of metrics gathered during evaluation.\n",
    "\n",
    "# Saving and Loading Policies\n",
    "\n",
    "Brax can save and load trained policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gOWeDqlP35sI"
   },
   "outputs": [],
   "source": [
    "model.save_params('/tmp/params', params)\n",
    "params = model.load_params('/tmp/params')\n",
    "inference_fn = make_inference_fn(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YlZvIG320sK"
   },
   "source": [
    "The trainers return an inference function, parameters, and the final set of metrics gathered during evaluation.\n",
    "\n",
    "# Saving and Loading Policies\n",
    "\n",
    "Brax can save and load trained policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "height": 480
    },
    "executionInfo": {
     "elapsed": 33520,
     "status": "ok",
     "timestamp": 1679346718844,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "kF5fS-yo35sI",
    "outputId": "94d1a7d5-8d6e-456c-8cfd-94a689b8808f"
   },
   "outputs": [],
   "source": [
    "#@title Visualizing a trajectory of the learned inference function\n",
    "\n",
    "# create an env with auto-reset\n",
    "env = envs.create(env_name=env_name, backend=backend)\n",
    "\n",
    "jit_env_reset = jax.jit(env.reset)\n",
    "jit_env_step = jax.jit(env.step)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "rollout = []\n",
    "rng = jax.random.PRNGKey(seed=1)\n",
    "state = jit_env_reset(rng=rng)\n",
    "for _ in range(1000):\n",
    "  rollout.append(state.pipeline_state)\n",
    "  act_rng, rng = jax.random.split(rng)\n",
    "  act, _ = jit_inference_fn(state.obs, act_rng)\n",
    "  state = jit_env_step(state, act)\n",
    "\n",
    "# HTML(html.render(env.sys.tree_replace({'opt.timestep': env.dt}), rollout))\n",
    "# # save html to file\n",
    "html_file = f\"{env_name}.html\"\n",
    "with open(html_file, 'w') as f:\n",
    "  f.write(html.render(env.sys.tree_replace({'opt.timestep': env.dt}), rollout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBtrAqns35sI"
   },
   "source": [
    "ðŸ™Œ See you soon!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
